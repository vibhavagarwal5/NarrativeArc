{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import util\n",
    "import random\n",
    "import embeddings\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = embeddings.load_glove()\n",
    "\n",
    "\n",
    "# language_model = embeddings.load_local_word2vec()\n",
    "# text = util.get_alphanumeral(collections[0][\"texts\"][0])\n",
    "# embeddings.get_doc_embedding(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = util.get_processed_data(\"./data/embeddings_gensim.csv\", False)\n",
    "collections = util.get_collections(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in collections:\n",
    "    lrs = []\n",
    "    for i,text in enumerate(col[\"texts\"]):\n",
    "        new_text = \"\"\n",
    "        for word in text.split():\n",
    "            if len(word)>1:\n",
    "                new_text+=word+\" \"\n",
    "        if new_text.strip()!=\"\":\n",
    "            lrs.append(new_text.strip().lower())\n",
    "    col[\"texts\"] = lrs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = []\n",
    "doc_to_ix = {}\n",
    "for col in collections:\n",
    "    for text in col[\"texts\"]:\n",
    "        if text not in doc_to_ix:\n",
    "            doc_to_ix[text] = len(doc_to_ix)\n",
    "            documents.append(text)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "word_to_ix = {word:index for index, word in enumerate(feature_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vibhavagarwal/miniconda2/envs/dl_torch_py2.7/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import util\n",
    "training_data, testing_data = util.get_train_test(collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_embedding(text, word_embedding_dim):\n",
    "    global vocab\n",
    "    global device\n",
    "    global tf_matrix\n",
    "    global word_to_ix\n",
    "    global doc_to_ix\n",
    "    embeds = []\n",
    "    text = text.lower()\n",
    "    for word in text.split():\n",
    "        if word in vocab:\n",
    "#             print(word, text)\n",
    "            tf_idf = tf_matrix[doc_to_ix[text], word_to_ix[word]]\n",
    "#             if tf_idf == 0:\n",
    "#                 print(\"0 tfidf: text\", text, \"word\", word)\n",
    "            embed = [v * tf_idf for v in vocab[word]]\n",
    "            embeds.append(torch.tensor(embed, dtype = torch.float, device = device))\n",
    "        else:\n",
    "            embeds.append(torch.zeros(word_embedding_dim, device = device, dtype = torch.float))\n",
    "    embeds = torch.cat(embeds).view(len(text.split()), -1)\n",
    "    return embeds.sum(dim = 0)/embeds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similarity(text1, text2):\n",
    "    cos = nn.CosineSimilarity(dim = 0)\n",
    "    embed1 = get_doc_embedding(text1,EMBEDDING_DIM)\n",
    "    embed2 = get_doc_embedding(text2,EMBEDDING_DIM)\n",
    "    \n",
    "    return cos(embed1, embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_similarity_score(texts):\n",
    "    count = 0\n",
    "    similarity_sum = 0.0\n",
    "    for ind in range(len(texts)-1):\n",
    "        similarity_sum += text_similarity(texts[ind],texts[ind+1])\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return similarity_sum/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, doc_embedding_dim, hidden_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(doc_embedding_dim, hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, 2)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1,1,self.hidden_dim, device = device), torch.zeros(1,1,self.hidden_dim, device = device))\n",
    "\n",
    "    def forward(self, coll_embeds):\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(coll_embeds.view(len(coll_embeds),1,-1), self.hidden)\n",
    "        out = self.hidden2out(lstm_out[-1])\n",
    "        score = F.log_softmax(out, dim = 1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "doc2vec_model= Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch ', 1, 0, 'percent complete')\n",
      "('Current accuracy: ', 0.0)\n",
      "('epoch ', 1, 5, 'percent complete')\n",
      "('Current accuracy: ', 0.7547169811320755)\n",
      "('epoch ', 1, 10, 'percent complete')\n",
      "('Current accuracy: ', 0.8535433070866142)\n",
      "('epoch ', 1, 15, 'percent complete')\n",
      "('Current accuracy: ', 0.8885383806519453)\n",
      "('epoch ', 1, 20, 'percent complete')\n",
      "('Current accuracy: ', 0.9014195583596214)\n",
      "('epoch ', 1, 25, 'percent complete')\n",
      "('Current accuracy: ', 0.910410094637224)\n",
      "('epoch ', 1, 30, 'percent complete')\n",
      "('Current accuracy: ', 0.9195160441872698)\n",
      "('epoch ', 1, 35, 'percent complete')\n",
      "('Current accuracy: ', 0.9220018034265104)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a65d68deb66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vibhavagarwal/miniconda2/envs/dl_torch_py2.7/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vibhavagarwal/miniconda2/envs/dl_torch_py2.7/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "loss_array = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    printed_percentages = []\n",
    "    corrects = 0\n",
    "    for collection,label in np.array(training_data):\n",
    "        \n",
    "        complete_percentage = int(count*100/len(training_data))\n",
    "        if complete_percentage%5 == 0 and (complete_percentage not in printed_percentages):\n",
    "            print(\"epoch \",epoch+1,complete_percentage,\"percent complete\")\n",
    "            print(\"Current accuracy: \",(corrects+0.0)/(count+1))\n",
    "            printed_percentages.append(complete_percentage)\n",
    "        count+=1\n",
    "        \n",
    "        label = torch.tensor([label], dtype = torch.long, device = device)\n",
    "#         collection_ixs = [prepare_sequence(text.split(),word_to_ix) for text in collection]\n",
    "#         collection_embeds =[get_doc_embedding(text, EMBEDDING_DIM) for text in collection]\n",
    "#         collection_embeds =[df[df.text==text]['embeddings'][0] for text in collection]\n",
    "        collection_embeds = [torch.tensor(doc2vec_model.infer_vector(text), dtype = torch.float, device = device) for text in collection]\n",
    "        collection_embeds = torch.cat(collection_embeds).view(len(collection), -1)\n",
    "        model.zero_grad()\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "#         print(collection_ixs)\n",
    "        \n",
    "        score = model(collection_embeds)\n",
    "        _, predicted = torch.max(score,1)\n",
    "        correct = 1 if (predicted == label) else 0\n",
    "        corrects += correct\n",
    "        \n",
    "#         print(score.shape,label.shape)\n",
    "        loss = loss_function(score, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_array.append(total_loss) \n",
    "    print(\"\\nepoch \"+str(epoch+1)+\" loss: \"+str(total_loss)+\"\\n\")\n",
    "    print(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "with torch.no_grad():\n",
    "    total_coll = len(testing_data)\n",
    "    correct_preds = 0\n",
    "\n",
    "    for collection,label  in np.array(testing_data):\n",
    "        label = torch.tensor(label, dtype = torch.long, device = device)\n",
    "        collection_embeds =[get_doc_embedding(text, EMBEDDING_DIM) for text in collection]\n",
    "        collection_embeds = torch.cat(collection_embeds).view(len(collection), -1)\n",
    "#         collection_ixs = [prepare_sequence(text.split(),word_to_ix) for text in collection]\n",
    "        score = model(collection_embeds)\n",
    "        _, predicted = torch.max(score,1)\n",
    "        correct = 1 if (predicted == label) else 0\n",
    "        correct_preds += correct\n",
    "                \n",
    "    print(\"Total collections : \"+str(total_coll))\n",
    "    print(\"Correct predictions: \"+str(correct_preds))\n",
    "    print (\"Accuracy : \"+str((correct_preds+0.0)/total_coll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('./data/collectionsWithTaxonomy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125564 entries, 0 to 125563\n",
      "Data columns (total 7 columns):\n",
      "collection_id    125564 non-null object\n",
      "sequence_id      125564 non-null int64\n",
      "resource_id      125564 non-null object\n",
      "taxonomy         75474 non-null object\n",
      "title            125312 non-null object\n",
      "description      113221 non-null object\n",
      "is_deleted       125564 non-null object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "d1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model,\"./models/word_lstm_collections_csv.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
