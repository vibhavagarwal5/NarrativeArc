{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses word level and learning resource level embeddings.\n",
    "# All embeddings are trained on the fly\n",
    "# Last hidden layer of word level LSTMs are concatenated with LR embeddings and fed into another LSTM\n",
    "# Last hidden layer of the second LSTM is used to classify the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import util\n",
    "import random\n",
    "# import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = embeddings.load_word2vec()\n",
    "# text = util.get_alphanumeral(collections[0][\"texts\"][0])\n",
    "# embeddings.get_doc_embedding(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = util.get_processed_data(\"./data/collections_math.csv\", True)\n",
    "collections = util.get_collections(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = util.get_train_test([col[\"texts\"] for col in collections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dictionaries for words and learning resources\n",
    "word_to_ix = {}\n",
    "text_to_ix = {}\n",
    "for col in collections:\n",
    "    for text in col[\"texts\"]:\n",
    "        if text not in text_to_ix:\n",
    "            text_to_ix[text] = len(text_to_ix)\n",
    "            for word in text.split():\n",
    "                if word not in word_to_ix:\n",
    "                        word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_EMBEDDING_DIM = 30\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "WORD_HIDDEN_DIM = 30\n",
    "SENT_HIDDEN_DIM = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_embedding_dim, sent_embedding_dim, word_hidden_dim, sent_hidden_dim, word_vocab_size, sent_vocab_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.word_hidden_dim = word_hidden_dim\n",
    "        self.sent_hidden_dim = sent_hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        self.sent_embeddings = nn.Embedding(sent_vocab_size, sent_embedding_dim)\n",
    "        self.word_lstm = nn.LSTM(word_embedding_dim, word_hidden_dim)\n",
    "        self.sent_lstm = nn.LSTM(word_hidden_dim + sent_embedding_dim, sent_hidden_dim)\n",
    "        \n",
    "        self.hidden2out = nn.Linear(sent_hidden_dim, 2)\n",
    "        self.sent_hidden = self.init_sent_hidden()\n",
    "        \n",
    "    def init_word_hidden(self):\n",
    "        return (torch.zeros(1,1,self.word_hidden_dim, device = device), torch.zeros(1,1,self.word_hidden_dim, device = device))\n",
    "    \n",
    "    def init_sent_hidden(self):\n",
    "        return (torch.zeros(1,1,self.sent_hidden_dim, device = device), torch.zeros(1,1,self.sent_hidden_dim, device = device))\n",
    "\n",
    "    def forward(self, collection, word_ixs):\n",
    "        for i,sent_ix in enumerate(collection):\n",
    "#             print(\"word_ixs: \",word_ixs)\n",
    "            self.word_hidden = self.init_word_hidden()\n",
    "            word_embeds = self.word_embeddings(word_ixs[i])\n",
    "            word_lstm_out, self.word_hidden = self.word_lstm(word_embeds.view(len(word_ixs[i]),1,-1), self.word_hidden)\n",
    "            \n",
    "            sent_embeds = self.sent_embeddings(torch.tensor([sent_ix], dtype = torch.long, device = device))\n",
    "            sent_lstm_input = torch.cat((sent_embeds.view(-1), self.word_hidden[0].view(-1))).view(1,1,-1)\n",
    "            out, self.sent_hidden = self.sent_lstm(sent_lstm_input, self.sent_hidden)\n",
    "            \n",
    "        out = self.hidden2out(self.sent_hidden[0].view(1,-1))\n",
    "        score = F.log_softmax(out, dim = 1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 0 percent complete\n",
      "epoch  1 5 percent complete\n",
      "epoch  1 10 percent complete\n",
      "epoch  1 15 percent complete\n",
      "epoch  1 20 percent complete\n",
      "epoch  1 25 percent complete\n",
      "epoch  1 30 percent complete\n",
      "epoch  1 35 percent complete\n",
      "epoch  1 40 percent complete\n",
      "epoch  1 45 percent complete\n",
      "epoch  1 50 percent complete\n",
      "epoch  1 55 percent complete\n",
      "epoch  1 60 percent complete\n",
      "epoch  1 65 percent complete\n",
      "epoch  1 70 percent complete\n",
      "epoch  1 75 percent complete\n",
      "epoch  1 80 percent complete\n",
      "epoch  1 85 percent complete\n",
      "epoch  1 90 percent complete\n",
      "epoch  1 95 percent complete\n",
      "\n",
      "epoch 1 loss: 3944.1760928034782\n",
      "\n",
      "epoch  2 0 percent complete\n",
      "epoch  2 5 percent complete\n",
      "epoch  2 10 percent complete\n",
      "epoch  2 15 percent complete\n",
      "epoch  2 20 percent complete\n",
      "epoch  2 25 percent complete\n",
      "epoch  2 30 percent complete\n",
      "epoch  2 35 percent complete\n",
      "epoch  2 40 percent complete\n",
      "epoch  2 45 percent complete\n",
      "epoch  2 50 percent complete\n",
      "epoch  2 55 percent complete\n",
      "epoch  2 60 percent complete\n",
      "epoch  2 65 percent complete\n",
      "epoch  2 70 percent complete\n",
      "epoch  2 75 percent complete\n",
      "epoch  2 80 percent complete\n",
      "epoch  2 85 percent complete\n",
      "epoch  2 90 percent complete\n",
      "epoch  2 95 percent complete\n",
      "\n",
      "epoch 2 loss: 3138.0048011541367\n",
      "\n",
      "epoch  3 0 percent complete\n",
      "epoch  3 5 percent complete\n",
      "epoch  3 10 percent complete\n",
      "epoch  3 15 percent complete\n",
      "epoch  3 20 percent complete\n",
      "epoch  3 25 percent complete\n",
      "epoch  3 30 percent complete\n",
      "epoch  3 35 percent complete\n",
      "epoch  3 40 percent complete\n",
      "epoch  3 45 percent complete\n",
      "epoch  3 50 percent complete\n",
      "epoch  3 55 percent complete\n",
      "epoch  3 60 percent complete\n",
      "epoch  3 65 percent complete\n",
      "epoch  3 70 percent complete\n",
      "epoch  3 75 percent complete\n",
      "epoch  3 80 percent complete\n",
      "epoch  3 85 percent complete\n",
      "epoch  3 90 percent complete\n",
      "epoch  3 95 percent complete\n",
      "\n",
      "epoch 3 loss: 2910.1693448126316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model = LSTMClassifier(WORD_EMBEDDING_DIM, SENT_EMBEDDING_DIM, WORD_HIDDEN_DIM, SENT_HIDDEN_DIM, len(word_to_ix), len(text_to_ix)).to(device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    printed_percentages = []\n",
    "    for collection,label in training_data:\n",
    "        \n",
    "        complete_percentage = int(count*100/len(training_data))\n",
    "        if complete_percentage%5 == 0 and (complete_percentage not in printed_percentages):\n",
    "            print(\"epoch \",epoch+1,complete_percentage,\"percent complete\")\n",
    "            printed_percentages.append(complete_percentage)\n",
    "        count+=1\n",
    "        \n",
    "        label = torch.tensor([label], dtype = torch.long, device = device)\n",
    "        sent_ixs = prepare_sequence(collection, text_to_ix)\n",
    "        word_ixs = [prepare_sequence(text.split(),word_to_ix) for text in collection]\n",
    "        model.zero_grad()\n",
    "\n",
    "        model.sent_hidden = model.init_sent_hidden()\n",
    "#         print(collection_ixs)\n",
    "        \n",
    "        score = model(sent_ixs, word_ixs)\n",
    "        loss = loss_function(score, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "    print(\"\\nepoch \"+str(epoch+1)+\" loss: \"+str(total_loss)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  0\n",
      "Total collections : 2716\n",
      "Correct predictions: 1359\n",
      "Accuracy : 0.5003681885125184\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "with torch.no_grad():\n",
    "    total_coll = len(testing_data)\n",
    "    correct_preds = 0\n",
    "    count = 0\n",
    "    for collection,label  in testing_data:\n",
    "        label = torch.tensor(label, dtype = torch.long, device = device)\n",
    "        sent_ixs = prepare_sequence(collection, text_to_ix)\n",
    "        word_ixs = [prepare_sequence(text.split(),word_to_ix) for text in collection]\n",
    "        score = model(sent_ixs, word_ixs)\n",
    "        _, predicted = torch.max(score,1)\n",
    "        correct = 1 if (predicted == label) else 0\n",
    "        correct_preds += correct\n",
    "        \n",
    "    print(\"Count: \",count)\n",
    "    print(\"Total collections : \"+str(total_coll))\n",
    "    print(\"Correct predictions: \"+str(correct_preds))\n",
    "    print (\"Accuracy : \"+str(correct_preds/total_coll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model,\"./models/word_lstm_collections_csv.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
