{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import util\n",
    "import ast\n",
    "import pickle\n",
    "import math\n",
    "import ast\n",
    "from scipy.spatial.distance import cosine\n",
    "import subprocess\n",
    "import random\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resource_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>creator_id</th>\n",
       "      <th>modifier_id</th>\n",
       "      <th>original_creator_id</th>\n",
       "      <th>original_content_id</th>\n",
       "      <th>parent_content_id</th>\n",
       "      <th>...</th>\n",
       "      <th>creator_system</th>\n",
       "      <th>tenant</th>\n",
       "      <th>tenant_root</th>\n",
       "      <th>primary_language</th>\n",
       "      <th>max_score</th>\n",
       "      <th>Summarization</th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_matrix</th>\n",
       "      <th>lda_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9dad3a1d-6f8b-4ef6-bc2c-b207d8d2ce1f</td>\n",
       "      <td>Table of Contents - Google Docs</td>\n",
       "      <td>https://docs.google.com/document/d/1e9X9BmWycf...</td>\n",
       "      <td>2014-09-23 03:16:24</td>\n",
       "      <td>2016-11-02 23:13:58.325</td>\n",
       "      <td>f3495a12-a0f4-4092-8cba-6817e9818f18</td>\n",
       "      <td>f3495a12-a0f4-4092-8cba-6817e9818f18</td>\n",
       "      <td>f3495a12-a0f4-4092-8cba-6817e9818f18</td>\n",
       "      <td>0c47ef6b-cdc1-45bc-8764-476f0d422566</td>\n",
       "      <td>0c47ef6b-cdc1-45bc-8764-476f0d422566</td>\n",
       "      <td>...</td>\n",
       "      <td>Gooru-2.0-v2</td>\n",
       "      <td>ba956a97-ae15-11e5-a302-f8a963065976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atom FCNElement C, M WSChemical Rxn FCNChem an...</td>\n",
       "      <td>Table Contents Google Docs Table Contents Goog...</td>\n",
       "      <td>['atom', 'fcnelement', 'c', 'wschemical', 'rxn...</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[(0, 0.0029411765), (1, 0.0029411765), (2, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            resource_id                            title  \\\n",
       "0  9dad3a1d-6f8b-4ef6-bc2c-b207d8d2ce1f  Table of Contents - Google Docs   \n",
       "\n",
       "                                                 url           created_at  \\\n",
       "0  https://docs.google.com/document/d/1e9X9BmWycf...  2014-09-23 03:16:24   \n",
       "\n",
       "                updated_at                            creator_id  \\\n",
       "0  2016-11-02 23:13:58.325  f3495a12-a0f4-4092-8cba-6817e9818f18   \n",
       "\n",
       "                            modifier_id                   original_creator_id  \\\n",
       "0  f3495a12-a0f4-4092-8cba-6817e9818f18  f3495a12-a0f4-4092-8cba-6817e9818f18   \n",
       "\n",
       "                    original_content_id                     parent_content_id  \\\n",
       "0  0c47ef6b-cdc1-45bc-8764-476f0d422566  0c47ef6b-cdc1-45bc-8764-476f0d422566   \n",
       "\n",
       "                         ...                         creator_system  \\\n",
       "0                        ...                           Gooru-2.0-v2   \n",
       "\n",
       "                                 tenant tenant_root primary_language  \\\n",
       "0  ba956a97-ae15-11e5-a302-f8a963065976         NaN                1   \n",
       "\n",
       "  max_score                                      Summarization  \\\n",
       "0       NaN  Atom FCNElement C, M WSChemical Rxn FCNChem an...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Table Contents Google Docs Table Contents Goog...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  ['atom', 'fcnelement', 'c', 'wschemical', 'rxn...   \n",
       "\n",
       "                                        clean_matrix  \\\n",
       "0  [(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1...   \n",
       "\n",
       "                                          lda_topics  \n",
       "0  [(0, 0.0029411765), (1, 0.0029411765), (2, 0.0...  \n",
       "\n",
       "[1 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  lodaing data\n",
    "\n",
    "df = pd.read_csv(\"./../WSL/Final_Data/collections_all_science_out-temp_lda.csv\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to clean the text data\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cleaniong summarizations\n",
    "\n",
    "df.Summarization.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "df.Summarization = df.Summarization.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting string of list to list\n",
    "df.lda_topics = df.lda_topics.apply(lambda x : ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  selecting required columns \n",
    "columns = [\"collection_id\",\"resource_id\",\"sequence_id\",\"title\",\"description\",\"Summarization\",\"text\",\"clean\",\"clean_matrix\",\"lda_topics\"]\n",
    "data = df[columns]\n",
    "data.sort_values(by=['collection_id','sequence_id'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  collection of all SAummariazations\n",
    "allSummarizations = data.Summarization.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading doc2vec models and functions for return embeddingsfor a paticular text\n",
    "\n",
    "d2v= Doc2Vec.load(\"./../WSL/Final_Data/doc2vec_100dim_science.model\")\n",
    "\n",
    "def Doc2vec(doc):\n",
    "    test_data = word_tokenize(doc.lower())\n",
    "    return d2v.infer_vector(test_data)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bukka/anaconda3/lib/python3.6/site-packages/gensim/utils.py:488: DeprecationWarning: Call to deprecated `cum_table` (Attribute will be removed in 4.0.0, use self.vocabulary.cum_table instead).\n",
      "  setattr(self, attrib, None)\n"
     ]
    }
   ],
   "source": [
    "# loading word2vec models and functions for return embeddingsfor a paticular text\n",
    "\n",
    "word2vec = Word2Vec.load('./../WSL/Final_Data/Word2Vec_100dim_science')\n",
    "def Word2vec(doc):\n",
    "    words=doc.split()\n",
    "    emb=np.zeros(100)\n",
    "    for word in words:\n",
    "        if(word in word2vec.wv.vocab):\n",
    "            emb = np.add(emb,word2vec[word])\n",
    "    if(len(doc)!=0) :       \n",
    "        return emb/len(words)\n",
    "    else:\n",
    "        return(np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading svm model\n",
    "\n",
    "pickle_in = open(\"./../WSL/Final_Data/Science_SVM_model.sav\",\"rb\")\n",
    "svm_fun = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lodaing lda model\n",
    "\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "fileObject = open('./../WSL/Final_Data/lda_science_dictionary.model','rb')  \n",
    "dictionary = pickle.load(fileObject)\n",
    "ldamodel = lda.load('./../WSL/Final_Data/lda_science.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model to get combined doc embeddings,word embeddings and lda distribution of a learning resources\n",
    "\n",
    "def get_embedding(text):\n",
    "    dv1 = Doc2vec(text)\n",
    "    wd1 = Word2vec(text)\n",
    "    clean_matrix1 = clean(text).split()\n",
    "    clean_matrix1 = dictionary.doc2bow(clean_matrix1)      \n",
    "    lda = ldamodel.get_document_topics(clean_matrix1,per_word_topics=True)[0] \n",
    "#     print(type(wd1))\n",
    "    data = []\n",
    "    data += list(wd1)\n",
    "    data += list(dv1)\n",
    "    for j in range(0,20):\n",
    "            data += [lda[j][1]]\n",
    "    return data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model reconstruction from JSON file\n",
    "with open('./../WSL/Final_Data/Validator_windowsize_3_archi.json', 'r') as f:\n",
    "    validator = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "validator.load_weights('./../WSL/Final_Data/Validator_windowsize_3_weights.h5')\n",
    "\n",
    "\n",
    "#  function to get validator score for three learning resources a,b and c\n",
    "def get_validator_score(a,b,c):\n",
    "    print(\"entered validator checking\")\n",
    "    temp=[]\n",
    "    frame=[]\n",
    "    data1=get_embedding(a)\n",
    "    data2=get_embedding(b)    \n",
    "    data3=get_embedding(c)    \n",
    "    \n",
    "    temp.append(data1)\n",
    "    temp.append(data2)\n",
    "    temp.append(data3)\n",
    "    frame.append(temp)\n",
    "    \n",
    "    score = validator.predict(np.reshape(frame,(1,3,220)))\n",
    "    print(\"left validator checking\")\n",
    "    return score[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate cosin similarity for two embedding\n",
    "def cosin(v1,v2):\n",
    "    if(LA.norm(v1)!=0 and LA.norm(v2)!=0):\n",
    "        return (np.dot(np.array(v1),np.array(v2))/(LA.norm(v1) * LA.norm(v2)))\n",
    "    else:\n",
    "        return 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# java -jar NoveltySemanticCoherence.jar  SinglePair\n",
    "\n",
    "# function to get SC and NV score of two learning resources\n",
    "def getNoveltySC(text1,text2):\n",
    "    cmd =['java','-jar','./NoveltySemanticCoherence.jar','SinglePair',text1,text2]\n",
    "    a = subprocess.call(cmd)\n",
    "    f = open(\"./data/pairwise/singlepairwise\", \"r\")\n",
    "    sc =float(f.readline()) \n",
    "    nv = float(f.readline())\n",
    "    return sc,nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3550"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  plotting cosine similarities of all consequtive learning resources\n",
    "\n",
    "cosine_similarites = []\n",
    "allCollections = list(set(data.collection_id.values))\n",
    "for i in allCollections:\n",
    "    texts = data[data.collection_id == i].Summarization.values \n",
    "    for i in range(0,len(texts)-1):\n",
    "        cosine_similarites.append(cosin(Doc2vec(texts[i]),Doc2vec(texts[i+1])))\n",
    "\n",
    "plt.hist(cosine_similarites, normed=True, bins=100)\n",
    "plt.ylabel('Cosine Similarites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  setting cosine threshold for picking neghibourhood for a lr \n",
    "#  vlaues is choose from above plt\n",
    "threshold = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which returs neighbourhood of a learning resource based on condition that if cosine similarity falls in range\n",
    "#  [thresold-0.05, threshold +0.05]\n",
    "def Neighbourhood(lr,col_list,thre):\n",
    "    sim = []\n",
    "    lr_DE = Doc2vec(lr)\n",
    "    for lr2 in col_list:\n",
    "        sim.append(cosin(lr_DE,Doc2vec(lr2)))\n",
    "\n",
    "    final = []\n",
    "    final_cosin = []\n",
    "    for i in range(0,len(col_list)):\n",
    "        if(thre-0.05<=sim[i] and sim[i]<thre+0.05):\n",
    "            final.append(col_list[i])\n",
    "            final_cosin.append(sim[i])\n",
    "    final = [x for _,x in sorted(zip(final_cosin,final))]        \n",
    "    final.reverse()\n",
    "    return final[:min(5,len(final))]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to predict similarity between two text using svm model\n",
    "\n",
    "def text_similarity_svm(text1, text2):\n",
    "    \n",
    "    dv1 = Doc2vec(text1)\n",
    "    dv2 = Doc2vec(text2)\n",
    "    \n",
    "    wd1 = Word2vec(text1)\n",
    "    wd2 = Word2vec(text1)\n",
    "    \n",
    "    ds = cosin(dv1,dv2)\n",
    "    ws = cosin(wd1,wd2)\n",
    "\n",
    "    clean_matrix1 = clean(text1).split()\n",
    "    clean_matrix2 = clean(text2).split() \n",
    "\n",
    "    clean_matrix1 = dictionary.doc2bow(clean_matrix1)  \n",
    "    clean_matrix2 = dictionary.doc2bow(clean_matrix2) \n",
    "    \n",
    "    lda_1 = ldamodel.get_document_topics(clean_matrix1,per_word_topics=True)[0] \n",
    "    lda_2 = ldamodel.get_document_topics(clean_matrix2,per_word_topics=True)[0] \n",
    "\n",
    "#     print(lda_1)\n",
    "#     lda_1 = clean_matrix1.apply(lambda x: ldamodel.get_document_topics(x,per_word_topics=True)[0])\n",
    "#     lda_2 = clean_matrix1.apply(lambda x: ldamodel.get_document_topics(x,per_word_topics=True)[0])\n",
    "\n",
    "    a_list=[]\n",
    "    b_list=[]\n",
    "    for i,j in lda_1:\n",
    "        a_list.append(j)\n",
    "\n",
    "    for i,j in lda_2:\n",
    "        b_list.append(j)\n",
    "    \n",
    "    a_list=np.array(a_list)\n",
    "    b_list=np.array(b_list)\n",
    "\n",
    "    kl = np.sum(np.where(a_list!=0,a_list*np.log(a_list/b_list),0))    \n",
    "    sc,nv =getNoveltySC(text1,text2)\n",
    "    \n",
    "    data=[]\n",
    "    \n",
    "    data.append(ds)\n",
    "    data.append(ws)\n",
    "    data.append(sc)\n",
    "    data.append(nv)\n",
    "    data.append(kl)\n",
    "        \n",
    "    similarity = svm_fun.predict_proba(np.array(data).reshape(1,5))[0][1]\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_similarity_svm(allSummarizations[0],allSummarizations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to given next learning resource based on candidates provided and similarity function provided \n",
    "#  function uses Neighbourhood function \n",
    "def getNextlr(pre_lr,candidates,similarity_function):\n",
    "    lr = pre_lr\n",
    "    simi_Scores = []\n",
    "    for i in candidates:\n",
    "#         print(type(i))\n",
    "        simi_Scores.append(similarity_function(lr,i))\n",
    "        \n",
    "    total = sum(simi_Scores)\n",
    "    \n",
    "    prob = [x/total for x in simi_Scores]\n",
    "    ran = np.random.choice(candidates, 1, p=prob)\n",
    "    return ran    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to generate collection which takes arguments as starting lr, candidates, pathway length to generate, \n",
    "#  similarity function to use\n",
    "\n",
    "#  this has backtracing of resources implemented \n",
    "\n",
    "\n",
    "def generate_collection(start_lr,allresources,pathway_length,threshold,similarity_function):\n",
    "    previous_lr = None\n",
    "    present_lr = start_lr\n",
    "    final_collection = [start_lr]\n",
    "    next_lr = None \n",
    "    validator = 0\n",
    "    i = 1\n",
    "    while(i<pathway_length):\n",
    "        nearto_present_lr = Neighbourhood(present_lr,allresources,threshold)        \n",
    "        if(i==1):\n",
    "            print(\"Value of i is:\",i)\n",
    "            next_lr = getNextlr(present_lr,nearto_present_lr,similarity_function)\n",
    "            if(len(next_lr)==0):return final_collection\n",
    "            else : next_lr = next_lr[0]\n",
    "            final_collection.append(next_lr)\n",
    "            previous_lr = present_lr\n",
    "            present_lr = next_lr\n",
    "            i+= 1\n",
    "        if(i>1):\n",
    "            while(len(nearto_present_lr)!=0):\n",
    "                print(\"Value of i is:\",i)\n",
    "                next_lr = getNextlr(present_lr,nearto_present_lr,similarity_function)\n",
    "                if(len(next_lr)==0):\n",
    "                    return final_collection\n",
    "                else:\n",
    "                    next_lr =next_lr[0]\n",
    "                score = get_validator_score(final_collection[i-2],final_collection[i-1],next_lr)\n",
    "                print(score)\n",
    "                if (score >= 0.70):\n",
    "                        final_collection.append(next_lr)\n",
    "                        previous_lr = present_lr\n",
    "                        present_lr = next_lr\n",
    "                        i+= 1\n",
    "                        break\n",
    "                elif (score < 0.70):\n",
    "                    nearto_present_lr.remove(next_lr)\n",
    "                    \n",
    "            if(len(nearto_present_lr)==0):\n",
    "                    present_lr = previous_lr\n",
    "                    previous_lr = final_collection[i-2]\n",
    "                    if(i>=1): \n",
    "                        i = i-1\n",
    "                        final_collection.pop(i)\n",
    "                    else: return final_collection\n",
    "    return final_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of i is: 1\n",
      "Value of i is: 2\n",
      "entered validator checking\n",
      "left validator checking\n",
      "0.7596993\n"
     ]
    }
   ],
   "source": [
    "#  generating a collection\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "col1 = generate_collection(allSummarizations[1],allSummarizations,3,threshold,text_similarity_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom composed subatomic particle proton neutron electronsthe chemical property element determined electron arrangedwhat suppose would cause atom lose electronnational standard benchmark science literacy 4dh1 atom made positively charged nucleus surrounded negatively charged electronsthe nucleus composed proton neutron roughly mass differ proton positively charged neutron electric charge snb op15 take 5 bullet note scroll click launch complete activity\n",
      "-------------------------------------------------\n",
      "element shell k l n p 2 ne 2 8 ar 2 8 8 kr 2 8 18 8 xe 2 8 18 18 8 rn 2 8 18 32 18 8 octet rule atom form ion combine compound obtain electron configuration nearest noble gas usually mean 8 outer electronsthe octet rule ion formation stated atom form ion seek electron configuration like nearest noble gasnearest refers number representative element element noble gasthere three way illustrate this q5 use three method chemical reaction equation br diagram lewis diagram show bond form between li cl mg follow octet rule 2e 8e 1e 11p 12n0 7e 8e 2e 17p 18n0 1e 2e 8e 11p 12n0 8e 8e 2e 17p 18n0 ionic bonding involves formation ion metal lose electron become positive ion nonmetal gain electron become negative ionslets look bond formed h f bohrrutherford 9p 10n0 9p 10n0 1p 0n0 1p 0n0 lewis diagram notice h f share electron able complete outer shell read text additional instruction learning objective necessary\n",
      "-------------------------------------------------\n",
      "atom number proton different number neutron called the neutral atom isotope element stuck step confusedto made mistake twovideo nuclear chemistry nuclear change nuclear reaction scientist atomic model atomic description bohr isotope notation proton neutron electron 164 dy summary page 3 5ms print take note notebook using main topic shown aligned cornell note\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# printing them\n",
    "for i in col1:\n",
    "    print(i)\n",
    "    print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
